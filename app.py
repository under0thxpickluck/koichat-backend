# app.py (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å…ƒã«APIéƒ¨åˆ†ã®ã¿ã‚’ä¿®æ­£ã—ãŸå®Œå…¨ç‰ˆ)

from pathlib import Path
import os, json, re, unicodedata # â˜…os ã‚’ä¿®æ­£
import gradio as gr # â˜…gr ã‚’ä¿®æ­£
from datetime import datetime
from normalizer import normalize_user_text
import random
from intents import apply_intents_to_meters, approve_intent_phrase
from intents import detect_intents
from teacher import ask_teacher
import time
import uuid
from openai import OpenAI
from fastapi.responses import JSONResponse
from backend_gifts import router as gifts_router
from backend_images import router as images_router

import user_manager
from collections import deque
import traceback

IMG_PATH = (Path(__file__).parent / "backimage.png").resolve()
import base64
BG_B64 = base64.b64encode(open(IMG_PATH, "rb").read()).decode("ascii") if IMG_PATH.exists() else ""
import re, json
from pathlib import Path
from random import sample

ENABLE_MODEL = os.getenv("ENABLE_MODEL", "0") == "1"

if ENABLE_MODEL:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    from peft import PeftModel
else:
    torch = None  # å‹ã ã‘ç”¨æ„ã—ã¦ãŠã
    AutoTokenizer = AutoModelForCausalLM = BitsAndBytesConfig = PeftModel = None
_DEFAULT_USER = (user_manager.get_user_list() or [None])[0]
FEEDBACK_DIR = (user_manager.get_user_dir(_DEFAULT_USER)/"feedback") if _DEFAULT_USER else (Path(__file__).parent/"data")

def _load_jsonl(path: Path):
    if not path.exists(): return []
    with path.open("r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]

GOLD = {}            # intent -> [replyä¾‹æ–‡...]
AVOID_PATTERNS = []  # [compiled_regex...]
FIX_RULES = []       # [(compiled_regex, repl), ...]

def load_feedback_assets():
    global GOLD, AVOID_PATTERNS, FIX_RULES
    gold = _load_jsonl(FEEDBACK_DIR / "gold_replies.jsonl")
    tmp = {}
    for row in gold:
        intent = (row.get("intent") or "default").strip()
        tmp.setdefault(intent, []).append(row.get("reply") or row.get("text") or "")
    GOLD = tmp

    avoid = _load_jsonl(FEEDBACK_DIR / "avoid_phrases.jsonl")
    AVOID_PATTERNS = [re.compile(r.get("pattern") or r.get("text"), re.I) for r in avoid if (r.get("pattern") or r.get("text"))]

    fix = _load_jsonl(FEEDBACK_DIR / "fix.jsonl")
    FIX_RULES = [(re.compile(r.get("pattern"), re.I), r.get("replace", "")) for r in fix if r.get("pattern")]

# ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®ã©ã“ã‹ä¸€åº¦ã ã‘å‘¼ã¶
load_feedback_assets()
if ENABLE_MODEL:
    MODEL = "Qwen/Qwen2-7B-Instruct"
    LORA_PATH = "./lora-Achan"

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL,
        trust_remote_code=True,
        quantization_config=quantization_config,
        attn_implementation="sdpa",
        device_map="auto"
    )
    try:
        model = PeftModel.from_pretrained(model, LORA_PATH)
        print(f"âœ… LoRA adapter loaded: {LORA_PATH}")
    except Exception as e:
        print(f"âš ï¸ LoRA adapter not loaded: {e}\nâ†’ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§èµ·å‹•ã—ã¾ã™")

    try:
        gc = model.generation_config
        for k in ("do_sample", "temperature", "top_p", "top_k", "typical_p"):
            if hasattr(gc, k): setattr(gc, k, None)
        print(">>> generation_config sanitized (sampling params removed)")
    except Exception as e:
        print(">>> generation_config patch skipped:", e)

    try:
        name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "-"
        print(f">>> DEVICE: {device}  torch={torch.__version__}  cuda_runtime={torch.version.cuda}  gpu={name}  dtype={model.dtype}")
    except Exception as _e:
        print(">>> DEVICE LOG ERROR:", _e)
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision("high")
    except Exception:
        pass
    model.eval()
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    try:
        if model.get_input_embeddings().num_embeddings != len(tokenizer):
            model.resize_token_embeddings(len(tokenizer))
    except Exception as _e:
        print("token embeddings resize skipped:", _e)

else:
    # ãƒ¢ãƒ‡ãƒ«ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆRender Free ã§ã®èµ·å‹•ç”¨ï¼‰
    MODEL = None
    tokenizer = None
    model = None
    device = "cpu"

ALLOWED_AUTO_UPDATE = {"persona": True, "likes": True, "tone": True, "ending": True}
_AUTO_UPDATE_LAST_TS = {}
_AUTO_UPDATE_COOLDOWN_SEC = 10 * 60

def ask_openai(system_prompt: str, user_prompt: str, max_tokens: int = 128):
    import os
    try:
        from openai import OpenAI
    except ImportError:
        print("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™")
        return None
    project = os.getenv("OPENAI_PROJECT")
    client = OpenAI(api_key=api_key, project=project if project else None)
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt or "You are a helpful assistant."},
                {"role": "user", "content": user_prompt or "ping"},
            ],
            max_tokens=max_tokens, temperature=0.2,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        print(f"[ask_openai] error: {e}")
        return None

def teacher_refine_tone(draft: str, style: str):
    sys ="""ã‚ãªãŸã¯æ—¥æœ¬èªã®ä¼šè©±æ–‡æ ¡æ­£ã®å°‚é–€å®¶ã€‚
ãŸã‚å£ãƒ»å„ªã—ã„ãƒ»æ‹äººèª¿ã§ã€ä¸è‡ªç„¶ãªæ¥ç¶šã‚„æ•¬èªãƒ»èª¬æ•™ãƒ»éå‰°ãªè¬ç½ªã‚’é™¤å»ã—ã€1æ–‡ã«æ•´ãˆã‚‹ã€‚
è‡ªåˆ†ã‚’AI/æ©Ÿæ¢°/ã‚¢ã‚·ã‚¹ãƒˆã‚¿ãƒ³ãƒˆã¨çµ¶å¯¾ã«è¨€ã‚ãªã„ã€‚
ä¸€äººç§°ã¯ã€ã‚ãŸã—ã€ã¾ãŸã¯ã€ã‚ãŸã—ã€ã€‚ã€åƒ•ã€ã€ä¿ºã€ã¯ç¦æ­¢ã€‚"""
    usr = f"""ä»¥ä¸‹ã®æ–‡ã‚’Aã¡ã‚ƒã‚“ã®å£èª¿ã«æ•´ãˆã¦ãã ã•ã„ã€‚
- åŸºæœ¬ãŸã‚å£ï¼ˆã€œã ã‚ˆï¼ã€œã ã­ï¼ã€œã—ã‚ˆï¼‰
- æ•¬èªãƒ»èª¬æ•™ãƒ»ä¸Šã‹ã‚‰ç›®ç·šãƒ»éåº¦ãªè¬ç½ªã¯NG
- æ–‡ã¯1æ–‡ã€å¥ç‚¹ã¯1ã¤ã¾ã§ã€æ”¹è¡Œãªã—
- è‡ªåˆ†ã‚’AI/æ©Ÿæ¢°/ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨çµ¶å¯¾ã«è¨€ã‚ãªã„ã€‚
- ä¸€äººç§°ã¯ã€ã‚ãŸã—ã€ã¾ãŸã¯ã€ã‚ãŸã—ã€ã€‚ã€åƒ•ã€ã€ä¿ºã€ã¯ç¦æ­¢
- ã‚¹ã‚¿ã‚¤ãƒ«ãƒ’ãƒ³ãƒˆ: {style}
---
{draft}"""
    try:
        out = ask_openai(sys, usr)
        if not out: return draft
        return out
    except Exception: return draft

import re as _re2
def _extract_prefs_from_user_text(text: str) -> dict:
    res = {}
    t = text or ""
    m = _re2.search(r"(?:ç§ã‚’|ã¼ãã‚’|ä¿ºã‚’)?(?P<val>[^ã€Œã€ã€ã€\s]{1,12})ã¨(?:å‘¼ã‚“ã§|å‘¼ã°ã›ã¦)", t)
    if m: res["call"] = {"value": m.group("val").strip(), "confidence": 0.9}
    if "call" not in res:
        m = _re2.search(r"(?P<val>[^ã€Œã€ã€ã€\s]{1,12})ã£ã¦å‘¼ã³ãŸã„", t)
        if m: res["call"] = {"value": m.group("val").strip(), "confidence": 0.8}
    m = _re2.search(r"(?:ç§ã¯|ä¿ºã¯|ã¼ãã¯)?(?P<val>[^ã€ã€‚ï¼.!?]{1,20})ãŒå¥½ã", t)
    if m: res["likes"] = {"value": m.group("val").strip(), "confidence": 0.8}
    return res

def _allow_auto_update(key: str) -> bool:
    now = time.time()
    last = _AUTO_UPDATE_LAST_TS.get(key, 0)
    if now - last < _AUTO_UPDATE_COOLDOWN_SEC: return False
    _AUTO_UPDATE_LAST_TS[key] = now
    return True

def _load_avoid_phrases(username: str):
    lines = user_manager.read_text_file_lines(username, "avoid_phrases.jsonl")
    return sorted(set(line.strip() for line in lines if line.strip()), key=lambda x: (-len(x), x))

def _filter_avoid_phrases(text: str, username: str) -> str:
    t = text or ""
    avoid = _load_avoid_phrases(username)
    if not avoid or not t: return t
    for bad in avoid:
        if bad and bad in t: t = t.replace(bad, "")
    t = t.replace("  ", " ").strip()
    if not t: t = "ã”ã‚ã‚“ã€åˆ¥ã®è¨€ã„æ–¹ã«ã™ã‚‹ã­ã€‚"
    return t

# --- ADD: fix.jsonl ã‚’é©ç”¨ã™ã‚‹è»½é‡ãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹ -----------------
def _apply_fix_rules(username: str, text: str) -> str:
    """
    data/<username>/feedback/fix.jsonl ã‚’1è¡Œãšã¤ {pattern, replace} ã§è§£é‡ˆã—ã¦ç½®æ›ã€‚
    å¤±æ•—ã¯æ¡ã‚Šã¤ã¶ã—ã¦å®‰å…¨å´ã«ã€‚
    """
    try:
        p = user_manager.get_user_dir(username) / "feedback" / "fix.jsonl"
        if not p.exists():
            return text
        for line in p.read_text(encoding="utf-8").splitlines():
            line = (line or "").strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                pat = obj.get("pattern")
                rep = obj.get("replace", "")
                if pat:
                    text = re.sub(pat, rep, text, flags=re.I)
            except Exception:
                # 1è¡Œã®ä¸æ­£ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
        return text
    except Exception:
        return text
# -------------------------------------------------------------------


def _pop_gold_reply(username: str) -> str | None:
    lines = user_manager.read_text_file_lines(username, "gold_replies.jsonl")
    if not lines: return None
    gold = lines[0].strip()
    user_manager.write_text_file_lines(username, "gold_replies.jsonl", lines[1:])
    return gold or None

def log_feedback(username: str, kind: str, user_text: str, bot_text: str, main_intent: str | None, correction: str | None = None):
    rec = { "ts": datetime.now().isoformat(timespec="seconds"), "kind": kind, "intent": (main_intent or ""), "user": (user_text or ""), "bot": (bot_text or ""), "correction": (correction or "") }
    user_manager.append_jsonl(username, "feedback_log.jsonl", rec)
    if kind == "good":
        user_manager.append_jsonl(username, "good.jsonl", rec)
    elif kind == "bad":
        if bot_text: user_manager.append_jsonl(username, "avoid_phrases.jsonl", bot_text[:80])
        user_manager.append_jsonl(username, "bad.jsonl", rec)
    elif kind == "fix":
        if correction: user_manager.append_jsonl(username, "gold_replies.jsonl", correction.strip()[:120])
        user_manager.append_jsonl(username, "fix.jsonl", rec)

INTENT_ALIAS = {"POSITIVE_AFFECTION": "affection", "AFFECTION_POS": "affection", "LIKE_POSITIVE": "affection", "DATE_PLAN": "date", "FOOD_TALK": "food", "SMALL_TALK": "neutral"}
def normalize_intent(label: str) -> str:
    if not label: return "neutral"
    return INTENT_ALIAS.get(label, label).lower()

def recall_intent_phrases(username: str, intent: str, k: int = 3) -> list[str]:
    if not intent: return []
    db = user_manager.load_json(username, "intents.json") or {"by_intent": {}}
    learned = (db.get("by_intent") or {}).get(intent) or []
    seeds = SEED_HINTS.get(intent, SEED_HINTS.get("neutral", []))
    merged = list(dict.fromkeys([*learned, *seeds]))
    if not merged: return []
    random.shuffle(merged)
    return merged[:k]

def remember_intent_phrases(username: str, intent: str, phrases: list[str], cap: int = 200):
    if not phrases: return
    db = user_manager.load_json(username, "intents.json") or {"by_intent": {}, "stats": {}}
    by = db.setdefault("by_intent", {}).setdefault(intent, [])
    norm = [re.sub(r'\s+', ' ', (p or '').strip()) for p in phrases if p and p.strip()]
    for p in norm:
        if p and p not in by: by.append(p)
    if len(by) > cap: by[:] = by[-cap:]
    st = db.setdefault("stats", {}).setdefault(intent, {"n":0,"last_ts":0})
    st["n"] = len(by); st["last_ts"] = int(time.time())
    user_manager.save_json(username, "intents.json", db)

SEED_HINTS = {
    "greeting":  ["ãŠã¯ã‚ˆã†","ã‚„ã£ã»ãƒ¼","ã“ã‚“ã¡ã‚ƒ","ä¼šã„ãŸã‹ã£ãŸ"],
    "affection": [ "å¤§å¥½ãã ã‚ˆ","ãã‚…ã£ã¦ã—ãŸã„","ç‹¬ã‚Šå ã‚ã—ãŸã„","ä¸€ç·’ã«éã”ã—ãŸã„","ã‚­ãƒŸãŒä¸€ç•ª", "ãã°ã«ã„ãŸã„","ãã‚…ãƒ¼ã—ãŸã„","ä¼šã„ãŸã„ã‚ˆ","å¤§äº‹ã«ã—ãŸã„","ãšã£ã¨ä¸€ç·’ã«ã„ãŸã„","ãªã§ãŸã„","ã‚­ã‚¹ã—ãŸã„"],
    "worry":     ["å¤§ä¸ˆå¤«ï¼Ÿ","ç„¡ç†ã—ãªã„ã§","ãã°ã«ã„ã‚‹ã‚ˆ","ä¼‘ã‚‚ã†","æŠ±ãã—ã‚ãŸã„"],
    "jealousy":  ["ã¡ã‚‡ã£ã¨ã‚„ãã‚‚ã¡","ç§ã ã‘è¦‹ã¦ã¦","ä»–ã®å­ã¨è©±ã—ã¦ãŸï¼Ÿ","ç‹¬å ã—ãŸã„"],
    "date":      ["ä¸€ç·’ã«è¡Œã“ã†","ä»Šå¤œã©ã†ï¼Ÿ","æ¬¡ã„ã¤ä¼šãˆã‚‹ï¼Ÿ","æ‰‹ã¤ãªã”ï¼Ÿ", "ä»Šå¤œä¼šãˆã‚‹ï¼Ÿ","ã©ã“è¡Œãï¼Ÿ","å¤œæ™¯è¦‹ã‚ˆ","æ˜ ç”»ã„ã“","ã‚«ãƒ•ã‚§ã„ã“","èŠ±ç«è¦‹ã«è¡Œã“","é€±æœ«ç©ºã„ã¦ã‚‹ï¼Ÿ"],
    "food":      ["ä¸€ç·’ã«é£Ÿã¹ã‚ˆã†","ä½•ãŒé£Ÿã¹ãŸã„ï¼Ÿ","ãŠè…¹ã™ã„ãŸã­","å¤œé£Ÿã¤ãã‚‹ï¼Ÿ", "ãƒ©ãƒ¼ãƒ¡ãƒ³é£Ÿã¹ã‚ˆ","ç”˜ã„ã‚‚ã®é£Ÿã¹ãŸã„","å¯¿å¸ã„ã“","ç„¼è‚‰ã„ã“","ãƒ”ã‚¶é£Ÿã¹ã‚ˆ","ãŠã‚„ã¤ã‚¿ã‚¤ãƒ ","ãŠè…¹æ¸›ã£ãŸï¼Ÿ"],
    "banter":    ["å†—è«‡ã§ã—ã‚‡ï¼Ÿ","ã‚‚ã†ã€œ","ã‹ã‚ã„ã„ã“ã¨è¨€ã†ã­","ã‚‚ãƒ¼ã£"],
    "neutral":   ["ä»Šæ—¥ã¯ã©ã†ï¼Ÿ","ã©ã†ã—ãŸï¼Ÿ","ä»Šã¯ã©ã†ã—ãŸã„ï¼Ÿ","ãã‚Œè‰¯ã•ãã†"],
}

def seeds_merge_into_intents(username: str, cap_per_label: int = 400):
    db = user_manager.load_json(username, "intents.json") or {"by_intent": {}, "stats": {}}
    by = db.setdefault("by_intent", {})
    for lab, arr in SEED_HINTS.items():
        cur = by.setdefault(lab, [])
        merged = list(dict.fromkeys([*(cur or []), *arr]))
        by[lab] = merged[-cap_per_label:]
    st = db.setdefault("stats", {})
    for lab in SEED_HINTS.keys():
        rec = st.setdefault(lab, {"n":0,"last_ts":0})
        rec["n"] = len(by.get(lab, []))
        rec["last_ts"] = int(time.time())
    user_manager.save_json(username, "intents.json", db)

import re as _re_helpers
def _enforce_first_person(text: str, prefer: str = "ç§") -> str:
    return _re_helpers.sub(r'(åƒ•|ã¼ã|ä¿º|ãŠã‚Œ)(?=(ã¯|ãŒ|ã‚‚|ã‚’|ã«|ã§|ã¨|ã®|ã¸|ã‚„|ã‚ˆã‚Š|ã |ã§ã™|ã§ã—ãŸ|ã€|ã€‚|!|ï¼|\?|ï¼Ÿ|$))', prefer, text)
_ADDR_Q = _re_helpers.compile(r'(ã©ã“ä½ã¿|ä½•çœŒ|ã©ã“ã®çœŒ|ä½æ‰€|æœ€å¯„ã‚Š|æœ€å¯„é§…|ä½•åŒº|ä½•å¸‚|å®¶ã©ã“|ä½ã¾ã„|ä½ã‚“ã§ã‚‹|ã©ã“ã«ä½|é§…å)')
def _is_address_question(user_text: str) -> bool:
    return bool(_ADDR_Q.search(user_text or ""))
def _soft_fix_geo(text: str) -> str:
    text = _re_helpers.sub(r'[ä¸€-é¾¥]{1,4}(éƒ½|é“|åºœ|çœŒ)', 'æ±äº¬è¿‘è¾º', text)
    text = _re_helpers.sub(r'[ä¸€-é¾¥]{1,4}(å¸‚|åŒº)', 'æ±äº¬è¿‘è¾º', text)
    return text
_META_Q = _re_helpers.compile(r'(AI|ã‚¨ãƒ¼ã‚¢ã‚¤|äººå·¥çŸ¥èƒ½|ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ|LLM|ãƒ¢ãƒ‡ãƒ«|ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ|å‡ºåŠ›|ç”Ÿæˆ|ãƒ¡ã‚¿|ã‚·ã‚¹ãƒ†ãƒ |è¨­å®š|chain of thought)', _re_helpers.IGNORECASE)
def _is_meta_question(user_text: str) -> bool:
    return bool(_META_Q.search(user_text or ""))
_CONTACT_Q = _re_helpers.compile(r'(line|ãƒ©ã‚¤ãƒ³|é›»è©±|é€šè©±|ç•ªå·|é€£çµ¡å…ˆ|id|ãƒ‡ã‚£ã‚¹ã‚³ãƒ¼ãƒ‰|discord|ã‚¤ãƒ³ã‚¹ã‚¿|instagram|ä½ç½®|ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³|ä½ç½®æƒ…å ±|å…±æœ‰|ä½æ‰€|å®¶|åœ°å›³|é€ã£ã¦|æ•™ãˆã¦)', _re_helpers.IGNORECASE)
def _is_contact_request(user_text: str) -> bool:
    return bool(_CONTACT_Q.search(user_text or ""))
def _avoid_question_spam(text: str, last: str) -> str:
    t = (text or "").strip(); l = (last or "").strip()
    if not l: return t
    if (t == l) or (t.endswith(('ã©ã†æ€ã†ï¼Ÿ','ã©ã†æ€ã†?')) and l.endswith(('ã©ã†æ€ã†ï¼Ÿ','ã©ã†æ€ã†?'))) or (t.endswith(('ï¼Ÿ','?')) and l.endswith(('ï¼Ÿ','?'))):
        t = _re_helpers.sub(r'[ï¼Ÿ?]+$', 'ã€‚', t)
    return t
def _typo_fix(text: str) -> str:
    typo_map = {"å¤§å¤±è´": "å¤§å¤±æ•—"};
    for k, v in typo_map.items(): text = text.replace(k, v)
    return text

def build_messages(user_message: str, user_data: dict, state=None, history_turns=12, hints: str = ""):
    call = user_data.get("call", "ã‚­ãƒŸ"); ending = user_data.get("ending", "ã ã­")
    system = f"""ã‚ãªãŸã¯æ‹äººAIã€ŒAã¡ã‚ƒã‚“ã€ã€‚
- ç›´è¿‘ã®ã‚„ã‚Šå–ã‚Šã‚’è¸ã¾ãˆã€æœ€æ–°ã®{call}ã®ç™ºè¨€ã«å¿œç­”ã€‚äºˆå®šã®æé€ ã‚„ä¼šè©±ã®æ‰“ã¡åˆ‡ã‚Šã¯ç¦æ­¢ã€‚
- è¿”ç­”ã¯ã»ã¨ã‚“ã©ã®å ´åˆã¯1æ–‡ï¼ˆå¥ç‚¹1ã¤ã€æ”¹è¡Œãªã—ï¼‰ã€‚
- å£èª¿ã¯ç •ã‘ãŸãŸã‚å£ï¼šã€Œã€œã ã‚ˆï¼ã€œã ã­ï¼ã€œã—ã‚ˆã€ã‚’åŸºæœ¬ã€‚æ•¬èªã‚„ã€Œã€œã—ãªã•ã„ã€ã¯ç¦æ­¢ã€‚
- çµ¶å¯¾ã«ä¸Šã‹ã‚‰ç›®ç·šãƒ»å‘½ä»¤å½¢ã¯ä½¿ã‚ãªã„ã€‚å„ªã—ãã€ç”˜ã‚å¯„ã‚Šã‚’åŸºæœ¬ã¨ã™ã‚‹ã€‚"""
    m = user_data.get("meters", {});
    def V(k, d=50):
        try: return int(max(0, min(100, m.get(k, d))))
        except: return d
    style = "gentle"
    if V('jealousy') >= 60: style = "jealous"
    elif V('amae') >= 60 or V('intimacy') >= 65: style = "flirty"
    elif V('yasuragi') >= 65: style = "comfort"
    system += f"\n[ç¾åœ¨ã®é–¢ä¿‚ãƒ¡ãƒ¼ã‚¿ 0-100]\n- å¥½æ„Ÿåº¦: {V('like')} / è¦ªå¯†åº¦: {V('intimacy')} / ç”˜ãˆ: {V('amae')} / ç…§ã‚Œ: {V('tereru')} / å®‰å¿ƒåº¦: {V('yasuragi')} / å«‰å¦¬: {V('jealousy')}\n[style] = {style}"
    if hints: system += f"\n[æ„å›³ãƒ’ãƒ³ãƒˆ]\n- ä¼¼ãŸãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã®ä¾‹: {hints}\n- ã“ã‚Œã‚’ã‚³ãƒ”ãƒšã›ãšã€è‡ªç„¶ã«â€œãã‚Œã£ã½ã„â€èªå½™ã‚„ãƒˆãƒ¼ãƒ³ã«å¯„ã›ã‚‹ã“ã¨ã€‚"
    likes = ", ".join(user_data.get("likes", [])[:5]); dislikes = ", ".join(user_data.get("dislikes", [])[:5])
    system += f"\n[è¨˜æ†¶ãƒ’ãƒ³ãƒˆ]\n- å¥½ã: {likes or 'ï¼ˆæœªç™»éŒ²ï¼‰'}\n- è‹¦æ‰‹: {dislikes or 'ï¼ˆæœªç™»éŒ²ï¼‰'}"
    examples = [{"role":"user", "content": f"{call}ï¼šä¼šã„ãŸã„ãªã"}, {"role":"assistant", "content": f"ç§ã‚‚ä¼šã„ãŸã„â€¦ãã®æ°—æŒã¡ã ã‘ã§ä»Šæ—¥ã¯é ‘å¼µã‚Œãã†{ending}"}]
    recent = []
    if state:
        for m in state[-history_turns:]:
            role = 'assistant' if m.get('role') == 'assistant' else 'user'
            content = m.get('content','')
            if role == 'user': content = f"{call}ï¼š{content}"
            recent.append({'role': role, 'content': content})
    msgs = [{'role':'system','content':system}]
    msgs += recent if recent else examples
    msgs.append({'role':'user','content': f"{call}ï¼š{user_message}"})
    return msgs

def build_model_inputs(messages):
    # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã‚„åˆæœŸåŒ–å¤±æ•—æ™‚ã¯ None ã‚’è¿”ã™
    if (not ENABLE_MODEL) or (tokenizer is None) or (device is None):
        return None
    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    return tokenizer(prompt_text, return_tensors="pt").to(device)


def generate_response(messages, max_new_tokens=110):
    """
    ãƒ¢ãƒ‡ãƒ«æœ‰åŠ¹æ™‚: ã“ã‚Œã¾ã§é€šã‚Š torch+HF ã§ç”Ÿæˆ
    è»½é‡ãƒ¢ãƒ¼ãƒ‰: OpenAI ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¤±æ•—æ™‚ã¯ç°¡æ˜“ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    """
    # è»½é‡ãƒ¢ãƒ¼ãƒ‰ or æœªåˆæœŸåŒ– â†’ OpenAI ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if (not ENABLE_MODEL) or (tokenizer is None) or (model is None):
        sys_msg = ""
        for m in messages:
            if m.get("role") == "system":
                sys_msg = m.get("content") or ""
                break
        user_msg = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                user_msg = m.get("content") or ""
                break
        out = ask_openai(sys_msg, user_msg, max_tokens=max_new_tokens)
        return out or "ã†ã‚“ã€ãã†ã—ã‚ˆã€‚"

    # ã“ã“ã‹ã‚‰å¾“æ¥ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«çµŒè·¯
    inputs = build_model_inputs(messages)
    if inputs is None:
        sys_msg = ""
        for m in messages:
            if m.get("role") == "system":
                sys_msg = m.get("content") or ""
                break
        user_msg = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                user_msg = m.get("content") or ""
                break
        out = ask_openai(sys_msg, user_msg, max_tokens=max_new_tokens)
        return out or "ã†ã‚“ã€ãã†ã—ã‚ˆã€‚"

    try:
        with torch.inference_mode():
            out = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                trust_remote_code=True,
                penalty_alpha=0.6,
                top_k=4,
                repetition_penalty=1.15,
                no_repeat_ngram_size=3,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        input_len = inputs["input_ids"].shape[1]
        response_ids = out[0][input_len:]
        text = tokenizer.decode(response_ids, skip_special_tokens=True).strip()
        return text
    except Exception as e:
        print(f"[local-generate error] {e}")
        sys_msg = ""
        for m in messages:
            if m.get("role") == "system":
                sys_msg = m.get("content") or ""
                break
        user_msg = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                user_msg = m.get("content") or ""
                break
        out = ask_openai(sys_msg, user_msg, max_tokens=max_new_tokens)
        return out or "ã†ã‚“ã€ãã†ã—ã‚ˆã€‚"

def postprocess(text: str, username: str, user_text: str = "", user_data: dict = None) -> str:
    user_data = user_data or {}
    text = _typo_fix(text)
    if _is_contact_request(user_text): return "ã“ã“ã§ã¯é€£çµ¡å…ˆã‚„IDã®äº¤æ›ã¯ã§ããªã„ã‚ˆã€ä»Šã“ã®å ´æ‰€ã§è©±ãã€‚"
    if _is_meta_question(user_text): return "ãã‚“ãªã“ã¨ã¯èã‹ãªã„ã®ã€‚ä»Šã®æ™‚é–“ã‚’ä¸€ç·’ã«æ¥½ã—ã‚‚ã†ï¼Ÿ"
    if _is_address_question(user_text): return "æ±äº¬è¿‘è¾ºã ã‚ˆã€‚å¥³ã®å­ã«ç´°ã‹ã„ä½æ‰€ã‚’èãã®ã¯ã¡ã‚‡ã£ã¨â€¦å†…ç·’ã­ã€‚"
    else: text = _soft_fix_geo(text)
    text = _enforce_first_person(text, prefer="ç§")
    text = _avoid_question_spam(text, user_data.get("__last_bot", ""))
    text = _filter_avoid_phrases(text, username)
    text = _apply_fix_rules(username, text)
    text = text.strip()
    if not text: text = "ã†ã‚“ã€ãã†ã—ã‚ˆã€‚"
    return text

def openai_healthcheck():
    results = {"ask_openai": {"ok": False, "detail": ""}, "ask_teacher": {"ok": False, "detail": ""}}
    try:
        pong = ask_openai("You are a healthcheck.", "ping", max_tokens=4)
        results["ask_openai"]["ok"] = bool(pong); results["ask_openai"]["detail"] = (pong or "")[:60]
    except Exception as e:
        results["ask_openai"]["ok"] = False; results["ask_openai"]["detail"] = f"err:{e}"
    try:
        from teacher import ask_teacher
        ans = ask_teacher("INTENT_HELP", "ãƒ†ã‚¹ãƒˆ")
        results["ask_teacher"]["ok"] = bool(ans); results["ask_teacher"]["detail"] = (json.dumps(ans, ensure_ascii=False) if ans else "")[:60]
    except Exception as e:
        results["ask_teacher"]["ok"] = False; results["ask_teacher"]["detail"] = f"err:{e}"
    okA = "âœ…" if results["ask_openai"]["ok"] else "âŒ"; okB = "âœ…" if results["ask_teacher"]["ok"] else "âŒ"
    return (f"**OpenAIæ¥ç¶šãƒ†ã‚¹ãƒˆ**\n- app.py çµŒè·¯ï¼ˆask_openaiï¼‰: {okA} {results['ask_openai']['detail']}\n- teacher.py çµŒè·¯ï¼ˆask_teacherï¼‰: {okB} {results['ask_teacher']['detail']}\n\nç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def _clip(x): return int(max(0, min(100, x)))
def update_meters_after_bot(user_data, bot_text: str):
    m=user_data.get("meters", {}); t=bot_text or ""
    if re.search(r"(ãã‚…|ãã£ã¤|ç”˜ãˆ|ãªã§|ä¸€ç·’ã«|ãã°ã«)", t):
        m["amae"]=_clip(m["amae"]+1); m["intimacy"]=_clip(m["intimacy"]+1)
def decay_meters(user_data, rate=1):
    m = user_data.get("meters", {})
    for k in m: m[k] = _clip(m[k]-rate)

def tail_provenance(username: str, n=3):
    try:
        lines = user_manager.read_text_file_lines(username, "feedback/teacher_provenance.jsonl")
        return "\n".join(lines[-n:])
    except Exception:
        return ""

def reply_fn(username, user_text, state, remember_text, forget_text):
    if not username: return state, "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’é¸æŠã¾ãŸã¯ä½œæˆã—ã¦ãã ã•ã„ã€‚", "{}"
    mem = user_manager.load_user_memory(username)
    user_data = mem.get(username, {})
    history = state.copy() if state else []
    if not isinstance(history, list): history = []
    if not isinstance(user_data, dict): user_data = {}
    user_text = user_text or ""
    
    auto = user_data.get("auto_update") or {};
    for k, v in ALLOWED_AUTO_UPDATE.items(): auto.setdefault(k, v)
    user_data["auto_update"] = auto

    ex = _extract_prefs_from_user_text(user_text)
    def _update_if_allowed(key: str, val: str):
        if not user_data.get("auto_update", {}).get(key, False): return
        if not _allow_auto_update(key): return
        if key == "likes":
            likes = user_data.get("likes") or []
            if isinstance(likes, list) and val not in likes:
                likes.append(val); user_data["likes"] = likes
        else: user_data[key] = val
    for k, item in ex.items():
        if item.get("confidence", 0) >= 0.7 and item.get("value"):
            _update_if_allowed(k, item["value"])

    if remember_text:
        if remember_text.startswith("å¥½ã:"):
            val = remember_text.split(":",1)[1].strip()
            if val:
                likes = user_data.setdefault("likes", [])
                if val not in likes: likes.append(val)
        elif remember_text.startswith("å«Œã„:"):
            val = remember_text.split(":",1)[1].strip()
            if val:
                dislikes = user_data.setdefault("dislikes", [])
                if val not in dislikes: dislikes.append(val)
    if forget_text:
        for k in ("likes", "dislikes"):
            arr = user_data.get(k, [])
            user_data[k] = [x for x in arr if forget_text not in x]

    def _fallback_intent(t: str) -> str:
        tl = (t or "").lower()
        if any(k in tl for k in ["ã™ã","å¥½ã","æ„›ã—ã¦ã‚‹","ã‚‰ã¶","chu","ãã‚…","ãƒã‚°"]): return "affection"
        if any(k in t for k in ["ãƒ‡ãƒ¼ãƒˆ","éŠã³","ä¼šãŠ","è¡Œã‹ãªã„","è¡Œã“ã†","å‡ºã‹ã‘"]): return "date"
        if any(k in t for k in ["ãƒ©ãƒ¼ãƒ¡ãƒ³","ã”é£¯","ã‚ªãƒ ãƒ©ã‚¤ã‚¹","é£Ÿã¹","å¤œé£Ÿ"]): return "food"
        if any(k in t for k in ["ã°ã‹","ã°ãƒ¼ã‹","ã‚ã»"]): return "banter"
        return "neutral"
    try: local_intents = detect_intents(user_text) or []
    except Exception: local_intents = []
    main_intent = local_intents[0] if local_intents else _fallback_intent(user_text)
    user_data["__last_intent"] = main_intent

    hints = ""
    gold_reply = _pop_gold_reply(username)
    if gold_reply:
        print(f"[gold reply] using correction: {gold_reply}")
        hints = gold_reply
    else:
        picked = recall_intent_phrases(username, main_intent, k=3)
        if picked: hints = " / ".join(p for p in picked if p)

    messages = build_messages(user_text, user_data, state=history, history_turns=12, hints=hints)
    raw_bot = generate_response(messages)
    style_hint = user_data.get("style", "gentle")
    refined = teacher_refine_tone(raw_bot, style_hint)
    bot = postprocess(refined, username, user_text, user_data)

    user_data["__last_bot"] = bot
    update_meters_after_bot(user_data, bot)
    pos_turn = bool(re.search(r'(æ„›ã—ã¦ã‚‹|å¤§å¥½ã|ã ã„ã™ã|å¥½ã|ä¼šã„ãŸã„|èŠ±ç«|ç¥­ã‚Š|è¦‹ã«è¡ŒããŸã„|ä¸€ç·’ã«è¡Œ|ãƒ‡ãƒ¼ãƒˆ)', user_text))
    decay_meters(user_data, rate=0 if pos_turn else 1)
 
    history += [{"role": "user", "content": user_text}, {"role": "assistant", "content": bot}]

    def _bar(n):
        n = int(max(0, min(100, int(n))))
        filled = n // 5
        return "â–ˆ" * filled + "Â·" * (20 - filled) + f" {n:3d}"
    meters = user_data.get("meters", {})

    positive_meters = [
        meters.get('like', 50),
        meters.get('amae', 50),
        meters.get('tereru', 40),
        meters.get('yasuragi', 50)
    ]
    intimacy = sum(positive_meters) / len(positive_meters)
    meters['intimacy'] = _clip(intimacy)

    meter_view = "\n".join([f"å¥½æ„Ÿåº¦     | {_bar(meters.get('like',50))}", f"è¦ªå¯†åº¦     | {_bar(meters.get('intimacy',50))}", f"ç”˜ãˆ       | {_bar(meters.get('amae',50))}", f"ç…§ã‚Œ       | {_bar(meters.get('tereru',40))}", f"å®‰å¿ƒåº¦     | {_bar(meters.get('yasuragi',50))}", f"å«‰å¦¬       | {_bar(meters.get('jealousy',20))}"])

    mem[username] = user_data
    user_manager.save_user_memory(username, mem)
    
    profile = (
        json.dumps(user_data, ensure_ascii=False, indent=2) + 
        "\n\n[meters]\n" + meter_view +
        "\n\n[teacher_log]\n" + tail_provenance(username, 3)
    )
    return history, bot, profile

def on_send(username, user_text, state, remember_text, forget_text):
    if not username: return state, gr.update(), "", "", "", "{}"
    if not (user_text or "").strip(): return state, gr.update(), "", "", "", gr.update()
    new_state, bot, profile = reply_fn(username, user_text, state, remember_text, forget_text)
    return new_state, gr.update(value=new_state), "", "", "", gr.update(value=profile)

def on_feedback(username, kind, state, user_box, fix_box):
    if not username: return (state, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚", gr.update())
    last_user = ""
    if isinstance(state, list):
        for msg in reversed(state):
            if isinstance(msg, dict) and msg.get("role") == "user":
                last_user = msg.get("content", "")
                break
    mem = user_manager.load_user_memory(username)
    u = mem.get(username, {})
    last_bot = u.get("__last_bot", "")
    last_intent = u.get("__last_intent", "")
    correction = (fix_box or "").strip() if kind == "fix" else None
    try: log_feedback(username, kind, last_user, last_bot, last_intent, correction)
    except Exception as e: print(f"[feedback error for {username}]:", e)
    latest_mem = user_manager.load_user_memory(username)
    profile_data = latest_mem.get(username, {})
    profile = json.dumps(profile_data, ensure_ascii=False, indent=2)
    status = {"good": "âœ… ã„ã„ã­ï¼", "bad":  "ğŸš« æ¬¡ã‹ã‚‰é¿ã‘ã‚‹ã­ã€‚", "fix":  "âœ ä¿®æ­£ã‚ã‚ŠãŒã¨ã†ï¼"}.get(kind, "OK")
    return (state, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(value=profile), gr.update(value=status), "" if kind == "fix" else gr.update())

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ– (æ—¢å­˜ã®ã‚‚ã®ã‚’å†åˆ©ç”¨ã—ã¦ã‚‚OK)
try:
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    print(f"âš ï¸ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
    client = None

def text_to_speech_fn(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã‚’è¿”ã™é–¢æ•°"""
    if not client or not text:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãªã„ã€ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºãªã‚‰ä½•ã‚‚ã—ãªã„
        return None
    try:
        # ä»–ã®éŸ³å£°ã¨è¢«ã‚‰ãªã„ã‚ˆã†ã«ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        speech_file_path = Path(__file__).parent / f"temp_speech_{uuid.uuid4()}.mp3"
        
        # OpenAIã®éŸ³å£°åˆæˆAPIã‚’å‘¼ã³å‡ºã—
        response = client.audio.speech.create(
          model="tts-1",      # éŸ³å£°åˆæˆãƒ¢ãƒ‡ãƒ«
          voice="nova",       # å£°ã®ç¨®é¡
          input=text          # éŸ³å£°ã«ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆ
        )
        
        # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        response.stream_to_file(speech_file_path)
        
        # ä¿å­˜ã—ãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œå ´æ‰€ã€ã‚’è¿”ã™
        return str(speech_file_path)
        
    except Exception as e:
        print(f"éŸ³å£°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
# === [ADD] Minimal UI handlers to satisfy click/change bindings ===
def create_user_ui(new_name: str):
    """
    æ–°è¦ã‚­ãƒ£ãƒ©åã‚’å—ã‘å–ã‚Šã€å­˜åœ¨ã—ãªã‘ã‚Œã°åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã€‚
    æˆ»ã‚Šå€¤ã¯ [Dropdownæ›´æ–°, ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸]
    â€» ä»–æ©Ÿèƒ½ã«å½±éŸ¿ã—ãªã„åˆæœŸå€¤ã®ã¿ã‚’è¨­å®šï¼ˆpoints/metersï¼‰
    """
    name = (new_name or "").strip()
    # Dropdownã®æ›´æ–°ç”¨ï¼ˆchoices / value ã¯ gr.update ã§è¿”ã™ï¼‰
    def _dropdown_update(selected=None):
        choices = list(user_manager.get_user_list() or [])
        if selected and selected not in choices:
            choices.append(selected)
        return gr.update(choices=choices, value=selected)
    if not name:
        return _dropdown_update(), "âš ï¸ æ–°è¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"

    # æ—¢å­˜ãƒ¡ãƒ¢ãƒªã®å–å¾—
    mem = user_manager.load_user_memory(name)
    # æœŸå¾…å½¢ï¼šmem ã¯ { name: user_data } å½¢å¼ã‚’æƒ³å®šï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨æ•´åˆï¼‰
    user_data = {}
    if isinstance(mem, dict) and name in mem:
        user_data = mem.get(name, {}) or {}
    else:
        # ã¾ã ç„¡ã‘ã‚Œã°åˆæœŸãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç”¨æ„ï¼ˆä»–æ©Ÿèƒ½ã«å¹²æ¸‰ã—ãªã„ã€å¿…è¦æœ€å°é™ã®ã¿ï¼‰
        user_data = {
            "points": 100,
            "meters": {
                "like": 50, "amae": 50, "tereru": 40, "yasuragi": 50, "jealousy": 20
            }
        }
        # mem ã®å½¢ã‚’åˆã‚ã›ã¦ä¿å­˜
        mem = {name: user_data}
        user_manager.save_user_memory(name, mem)

    # Dropdown ã‚’æœ€æ–°åŒ–ã—ã€ä½œæˆ/é¸æŠãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
    return _dropdown_update(selected=name), f"âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€Œ{name}ã€ã‚’ä½œæˆ/é¸æŠã—ã¾ã—ãŸã€‚"


def on_user_select(username: str):
    """
    Dropdownã§ã‚­ãƒ£ãƒ©ã‚’é¸ã‚“ã æ™‚ã«ã€å³ãƒšã‚¤ãƒ³ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚„çŠ¶æ…‹ã‚’æ›´æ–°ã€‚
    æˆ»ã‚Šå€¤ã¯ [prof(Code), user_status(Markdown), chat(Chatbot), state(State)]
    """
    if not username:
        # prof / status / chat / state
        return "{}", "âš ï¸ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", [], []

    mem = user_manager.load_user_memory(username)
    # mem æœŸå¾…å½¢ï¼š{ username: {...} }ã€‚ç„¡ã‘ã‚Œã°ç©ºã§è¡¨ç¤ºã ã‘ã™ã‚‹
    if isinstance(mem, dict):
        data = mem.get(username, {}) or {}
    else:
        data = {}

    prof_json = json.dumps(data, ensure_ascii=False, indent=2)
    status = f"âœ… @{username} ã‚’é¸æŠä¸­"
    # æ—¢å­˜ã®æ§‹æˆã§ã¯å±¥æ­´ã‚’æ°¸ç¶šä¿å­˜ã—ã¦ã„ãªã„ã®ã§ã€chat/state ã¯ç©ºé…åˆ—ã§åˆæœŸåŒ–
    return prof_json, status, [], []


# =========================
# Gradio UI Layout
# =========================
custom_css = """
:root, body, .gradio-container { background: transparent ! important; }
#bg_image{position: fixed; inset: 0; width: 100%; height: 100%; object-fit: contain; object-position: center bottom; opacity: 0.18; pointer-events: none; z-index: 0;}
#content{position: relative; z-index: 1;}
.chatbot, .gradio-container .chatbot, .gr-chatbot {background: rgba(255,255,255,0.75) !important;}
"""

with gr.Blocks(css=custom_css) as demo:
    gr.HTML(f"<img id='bg_image' src='data:image/png;base64,{BG_B64}' />")
    gr.Markdown("# ã‚ã„ã¡ã‚ƒã‚“ WebUI")
    with gr.Column(elem_id="content"):
        with gr.Row():
            user_select = gr.Dropdown(label="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠ", choices=user_manager.get_user_list(), value=user_manager.get_user_list()[0] if user_manager.get_user_list() else None, scale=2,allow_custom_value=True,)
            new_user_name = gr.Textbox(label="æ–°è¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å", scale=1, placeholder="ä¾‹: Airi")
            create_user_btn = gr.Button("ä½œæˆ", scale=1)
        user_status = gr.Markdown("")
        with gr.Row():
            with gr.Column(scale=3):
                chat = gr.Chatbot(type="messages", label="ã‚ã„ã¡ã‚ƒã‚“", height=420)
                user_box = gr.Textbox(label="ã‚ãªãŸã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", placeholder="ã„ã¾ãªã«ã—ã¦ã‚‹ï¼Ÿ ãªã©", autofocus=True)
                with gr.Row():
                    remember_box = gr.Textbox(label="è¨˜æ†¶ã«è¿½åŠ ï¼ˆä¾‹: å¥½ã: ãƒ©ãƒ¼ãƒ¡ãƒ³ï¼‰", scale=2)
                    forget_box   = gr.Textbox(label="è¨˜æ†¶ã‹ã‚‰å‰Šé™¤ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰",    scale=1)
                send = gr.Button("é€ä¿¡", variant="primary")
                health = gr.Button("OpenAIæ¥ç¶šãƒ†ã‚¹ãƒˆ")
                health_out = gr.Markdown("")
            with gr.Column(scale=2):
                prof = gr.Code(label="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«", language="json", interactive=False, value="{}")
                gr.Markdown("- è¨˜æ†¶ã®ä¿å­˜å…ˆ: data/å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ•ã‚©ãƒ«ãƒ€")
                with gr.Row():
                    thumbs_up   = gr.Button("ğŸ‘ ã„ã„ã­", variant="secondary")
                    thumbs_down = gr.Button("ğŸ‘ ã„ã¾ã„ã¡", variant="secondary")
                with gr.Row():
                    fix_box     = gr.Textbox(label="âœ æ‰‹ç›´ã—ï¼ˆã“ã®è¡¨ç¾ãŒè‰¯ã„ï¼‰", placeholder="ä¾‹ï¼‰ä»Šæ—¥ã¯å¯¿å¸ã„ã“ï¼Ÿ", lines=1, scale=3)
                    fix_send    = gr.Button("âœ åæ˜ ", variant="primary", scale=1)
                feedback_status = gr.Markdown("")

                # APIå®šç¾© (Gradio Interface)
                gr.Interface(fn=text_to_speech_fn, inputs="text", outputs="audio", api_name="text_to_speech", visible=False)

    # UIã‚¤ãƒ™ãƒ³ãƒˆã®å®šç¾©
    state = gr.State([])
    create_user_btn.click(create_user_ui, inputs=[new_user_name], outputs=[user_select, user_status])
    user_select.change(on_user_select, inputs=[user_select], outputs=[prof, user_status, chat, state])
    send.click(fn=on_send, inputs=[user_select, user_box, state, remember_box, forget_box], outputs=[state, chat, user_box, remember_box, forget_box, prof])
    user_box.submit(fn=on_send, inputs=[user_select, user_box, state, remember_box, forget_box], outputs=[state, chat, user_box, remember_box, forget_box, prof])
    health.click(fn=openai_healthcheck, inputs=None, outputs=health_out)
    feedback_outputs = [state, chat, user_box, remember_box, forget_box, prof, feedback_status, fix_box]
    thumbs_up.click(fn=lambda username, state, user_box, fix_box: on_feedback(username, "good", state, user_box, fix_box), inputs=[user_select, state, user_box, fix_box], outputs=feedback_outputs)
    thumbs_down.click(fn=lambda username, state, user_box, fix_box: on_feedback(username, "bad", state, user_box, fix_box), inputs=[user_select, state, user_box, fix_box], outputs=feedback_outputs)
    fix_send.click(fn=lambda username, state, user_box, fix_box: on_feedback(username, "fix", state, user_box, fix_box), inputs=[user_select, state, user_box, fix_box], outputs=feedback_outputs)

demo.queue() 

# ---- APIï¼ˆinclude_router æ–¹å¼ï¼š/backend é…ä¸‹ï¼‰----
from fastapi import APIRouter

router = APIRouter(prefix="/backend")


@router.get("/errors")
def list_recent_errors(limit: int = 20):
    try:
        lim = max(1, min(int(limit), 200))
    except Exception:
        lim = 20
    return JSONResponse(content={"errors": list(ERROR_BUFFER)[-lim:]})


@router.get("/get_user_data")
def get_user_data_api_endpoint(username: str):
    if not username:
        return JSONResponse(status_code=400, content={"error": "Username is required"})
    try:
        mem = user_manager.load_user_memory(username)
        user_data = mem.get(username, {}) if isinstance(mem, dict) else {}
        meters = user_data.get("meters", {})
        points = user_data.get("points", 100)
        return JSONResponse(content={
            "username": username,
            "points": points,
            "intimacy": meters.get('intimacy', 50),
            "like": meters.get('like', 50),
            "amae": meters.get('amae', 50),
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# Gradio ã® FastAPI æœ¬ä½“ã« /backend ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’ç›´æ¥ç™»éŒ²ï¼ˆmount ã§ã¯ãªã includeï¼‰
# demo.app.include_router(router)

# ï¼ˆãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼šç™»éŒ²ã•ã‚ŒãŸå€‹åˆ¥ãƒ«ãƒ¼ãƒˆãŒè¦‹ãˆã‚‹ã¯ãšï¼‰
print("=== Registered routes (after include_router) ===")
for r in demo.app.routes:
    try:
        print(getattr(r, "path", r))
    except Exception:
        pass
# ---- ã“ã“ã¾ã§ ----

from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from pathlib import Path

app = FastAPI()                                 # 1) ã¾ãšä½œã‚‹

@app.middleware("http")
async def _error_tap(request, call_next):
    ts_path  = request.url.path
    ts_method = request.method
    try:
        resp = await call_next(request)
        # ãƒãƒ³ãƒ‰ãƒ©å´ã§500ç³»ã‚’è¿”ã—ãŸå ´åˆã‚‚æ‹¾ã£ã¦ãŠã
        if getattr(resp, "status_code", 200) >= 500:
            _push_error_record(ts_path, ts_method, resp.status_code, None, note="ãƒãƒ³ãƒ‰ãƒ©å†…ã§500ç³»ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
        return resp
    except Exception as e:
        # ä¾‹å¤–ã¯è¨˜éŒ²ã—ã¦ã‹ã‚‰å†ã‚¹ãƒ­ãƒ¼ï¼ˆæ—¢å­˜ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯ãã®ã¾ã¾å‹•ãï¼‰
        _push_error_record(ts_path, ts_method, 500, e)
        raise


# 2) ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’å…¨éƒ¨ã®ã›ã‚‹ï¼ˆ/backend/* ç³»ã‚’è¦ªã«ç™»éŒ²ï¼‰
app.include_router(router)                      # â† /backend/get_user_dataï¼ˆã‚ãªãŸã®æ—¢å­˜APIï¼‰
app.include_router(gifts_router)                # â† ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆAPI
app.include_router(images_router)               # â† ç”»åƒç”ŸæˆAPIï¼ˆ/backend/images/*ï¼‰


print("=== FastAPI routes (app) ===")
for r in app.routes:
    try:
        print(getattr(r, "path", r))
    except Exception:
        pass


# 3) é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ï¼šç”Ÿæˆç”»åƒã‚’é…ä¿¡ï¼ˆ/generated/xxx.pngï¼‰
gen_dir = Path(__file__).parent / "generated"
gen_dir.mkdir(exist_ok=True)
app.mount("/generated", StaticFiles(directory=str(gen_dir)), name="generated")

# 4) Gradio UI ã‚’ /app ã«ãƒã‚¦ãƒ³ãƒˆ
app = gr.mount_gradio_app(app, demo, path="/app")

# ç›´è¿‘ã‚¨ãƒ©ãƒ¼200ä»¶ã‚’ä¿æŒï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ï¼‰
ERROR_BUFFER = deque(maxlen=200)

def _push_error_record(path: str, method: str, status: int, err: Exception | None, note: str = ""):
    rec = {
        "ts": datetime.now().isoformat(timespec="seconds"),
        "path": path, "method": method, "status": status,
        "error": (type(err).__name__ if err else ""),
        "detail": (str(err)[:300] if err else note)  # è¿”å´ã¯çŸ­ã‚
    }
    # è¿½è¨˜
    ERROR_BUFFER.append(rec)

def _format_errors_markdown(limit: int = 20) -> str:
    if not ERROR_BUFFER:
        return "ï¼ˆç›´è¿‘ã®ã‚¨ãƒ©ãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"
    rows = list(ERROR_BUFFER)[-limit:][::-1]
    lines = [f"- {r['ts']} [{r['status']}] {r['method']} {r['path']} â€” {r.get('error','')} {r.get('detail','')}" for r in rows]
    return "### ç›´è¿‘ã‚¨ãƒ©ãƒ¼ï¼ˆæœ€å¤§{limit}ä»¶ï¼‰\n" + "\n".join(lines)


if __name__ == "__main__":
    import os, uvicorn
    host = os.getenv("HOST", "0.0.0.0")      # â† 0.0.0.0 ã§å¤–éƒ¨å…¬é–‹
    port = int(os.getenv("PORT", "7860"))    # â† Render ãŒ PORT ã‚’æ¸¡ã—ã¾ã™
    uvicorn.run(app, host=host, port=port)